{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Import ML packages\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.naive_bayes import GaussianNB,MultinomialNB\n",
    "from sklearn.metrics import accuracy_score, hamming_loss, classification_report\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.model_selection import train_test_split\n",
    "from skmultilearn.problem_transform import BinaryRelevance\n",
    "from skmultilearn.problem_transform import ClassifierChain\n",
    "from skmultilearn.problem_transform import LabelPowerset\n",
    "from skmultilearn.adapt import MLkNN\n",
    "from sklearn.multioutput import MultiOutputClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from imblearn.under_sampling import RandomUnderSampler\n",
    "from imblearn.over_sampling import RandomOverSampler\n",
    "from sklearn.datasets import make_multilabel_classification\n",
    "from imblearn.combine import SMOTEENN\n",
    "from xgboost import XGBClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Lemmatized_Comment</th>\n",
       "      <th>Sexual_Type1</th>\n",
       "      <th>Sexual_Type2</th>\n",
       "      <th>Physical_Appearance</th>\n",
       "      <th>Race</th>\n",
       "      <th>Intellectual</th>\n",
       "      <th>General_Hate</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>zany</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>larry</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>get gif cannot have phone</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>larry zany sexy niall liam something stupid back</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>pretty much</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>men</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>larry</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>looking lady</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>larry little moment Smile</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>actually fan one direction larry hard tell tbe</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>laughing as eleanor shipper mean bitch laughin...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>really</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>bitch saying people seem care larry nowadays</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>ad</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>motherfucker need jesus love boy</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                   Lemmatized_Comment  Sexual_Type1   \n",
       "0                                                zany           0.0  \\\n",
       "1                                               larry           0.0   \n",
       "2                           get gif cannot have phone           0.0   \n",
       "3    larry zany sexy niall liam something stupid back           0.0   \n",
       "4                                         pretty much           0.0   \n",
       "5                                                 men           0.0   \n",
       "6                                               larry           0.0   \n",
       "7                                        looking lady           0.0   \n",
       "8                           larry little moment Smile           0.0   \n",
       "9      actually fan one direction larry hard tell tbe           0.0   \n",
       "10  laughing as eleanor shipper mean bitch laughin...           0.0   \n",
       "11                                             really           0.0   \n",
       "12       bitch saying people seem care larry nowadays           0.0   \n",
       "13                                                 ad           0.0   \n",
       "14                   motherfucker need jesus love boy           0.0   \n",
       "\n",
       "    Sexual_Type2  Physical_Appearance  Race  Intellectual  General_Hate  \n",
       "0            0.0                  0.0   0.0           0.0           0.0  \n",
       "1            0.0                  0.0   0.0           0.0           0.0  \n",
       "2            0.0                  0.0   0.0           0.0           0.0  \n",
       "3            0.0                  0.0   0.0           0.0           0.0  \n",
       "4            0.0                  0.0   0.0           0.0           0.0  \n",
       "5            0.0                  0.0   0.0           0.0           0.0  \n",
       "6            0.0                  0.0   0.0           0.0           0.0  \n",
       "7            0.0                  0.0   0.0           0.0           0.0  \n",
       "8            0.0                  0.0   0.0           0.0           0.0  \n",
       "9            0.0                  0.0   0.0           0.0           0.0  \n",
       "10           0.0                  0.0   0.0           0.0           1.0  \n",
       "11           0.0                  0.0   0.0           0.0           0.0  \n",
       "12           0.0                  0.0   0.0           0.0           0.0  \n",
       "13           0.0                  0.0   0.0           0.0           0.0  \n",
       "14           0.0                  0.0   0.0           0.0           0.0  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load Dataset\n",
    "df=pd.read_csv(r'D:/Machine Learning Group Project/balanced_data_type2.csv')\n",
    "df = df.drop(df.columns[0], axis=1)\n",
    "df.head(15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Lemmatized_Comment      object\n",
       "Sexual_Type1           float64\n",
       "Sexual_Type2           float64\n",
       "Physical_Appearance    float64\n",
       "Race                   float64\n",
       "Intellectual           float64\n",
       "General_Hate           float64\n",
       "dtype: object"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Check on types of data\n",
    "df.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-2 {color: black;}#sk-container-id-2 pre{padding: 0;}#sk-container-id-2 div.sk-toggleable {background-color: white;}#sk-container-id-2 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-2 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-2 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-2 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-2 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-2 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-2 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-2 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-2 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-2 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-2 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-2 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-2 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-2 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-2 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-2 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-2 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-2 div.sk-item {position: relative;z-index: 1;}#sk-container-id-2 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-2 div.sk-item::before, #sk-container-id-2 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-2 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-2 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-2 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-2 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-2 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-2 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-2 div.sk-label-container {text-align: center;}#sk-container-id-2 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-2 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-2\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>TfidfVectorizer(ngram_range=(1, 3), strip_accents=&#x27;unicode&#x27;)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-2\" type=\"checkbox\" checked><label for=\"sk-estimator-id-2\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">TfidfVectorizer</label><div class=\"sk-toggleable__content\"><pre>TfidfVectorizer(ngram_range=(1, 3), strip_accents=&#x27;unicode&#x27;)</pre></div></div></div></div></div>"
      ],
      "text/plain": [
       "TfidfVectorizer(ngram_range=(1, 3), strip_accents='unicode')"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X = df[\"Lemmatized_Comment\"]\n",
    "y= df[df.columns[1:]]\n",
    "\n",
    "vectorizer = TfidfVectorizer(strip_accents='unicode', analyzer='word', ngram_range=(1,3), norm='l2')\n",
    "vectorizer.fit(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(29241,)\n",
      "(12532,)\n",
      "(29241, 6)\n",
      "(12532, 6)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=101)\n",
    "print(X_train.shape)\n",
    "print(X_test.shape)\n",
    "print(y_train.shape)\n",
    "print(y_test.shape)\n",
    "X_train_tfidf = vectorizer.transform(X_train)\n",
    "X_test_tfidf = vectorizer.transform(X_test)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy Score:  0.757022023619534\n",
      "Hamming Loss:  0.06\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.88      0.66      0.75      1857\n",
      "           1       0.82      0.82      0.82      1826\n",
      "           2       0.86      0.70      0.77      1919\n",
      "           3       0.79      0.76      0.77      1831\n",
      "           4       0.87      0.80      0.83      1791\n",
      "           5       0.98      0.60      0.74      1754\n",
      "\n",
      "   micro avg       0.86      0.72      0.78     10978\n",
      "   macro avg       0.87      0.72      0.78     10978\n",
      "weighted avg       0.87      0.72      0.78     10978\n",
      " samples avg       0.90      0.76      0.86     10978\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Train for Linear Regression\n",
    "from sklearn.multioutput import MultiOutputClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "LR = MultiOutputClassifier(LogisticRegression()).fit(X_train_tfidf, y_train)\n",
    "prediction_LR = LR.predict(X_test_tfidf)\n",
    "prediction_LR\n",
    "#Check for accuracy.\n",
    "from sklearn.metrics import accuracy_score\n",
    "print('Accuracy Score: ', accuracy_score(y_test, prediction_LR))\n",
    "#Check for hamming loss\n",
    "from sklearn.metrics import hamming_loss\n",
    "print('Hamming Loss: ', round(hamming_loss(y_test, prediction_LR),2))\n",
    "#Check for precision, recall, f1-score.\n",
    "print(classification_report(y_test, prediction_LR, zero_division=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy Score:  0.7022023619533992\n",
      "Hamming Loss:  0.09\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.69      0.67      0.68      1857\n",
      "           1       0.69      0.82      0.75      1826\n",
      "           2       0.68      0.78      0.73      1919\n",
      "           3       0.65      0.80      0.72      1831\n",
      "           4       0.71      0.86      0.78      1791\n",
      "           5       0.72      0.56      0.63      1754\n",
      "\n",
      "   micro avg       0.69      0.75      0.72     10978\n",
      "   macro avg       0.69      0.75      0.71     10978\n",
      "weighted avg       0.69      0.75      0.71     10978\n",
      " samples avg       0.82      0.79      0.85     10978\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Train for Naive Bayes\n",
    "NB = MultiOutputClassifier(MultinomialNB()).fit(X_train_tfidf, y_train)\n",
    "prediction_NB = NB.predict(X_test_tfidf)\n",
    "prediction_NB\n",
    "#Check for accuracy.\n",
    "from sklearn.metrics import accuracy_score\n",
    "print('Accuracy Score: ', accuracy_score(y_test, prediction_NB))\n",
    "#Check for hamming loss\n",
    "from sklearn.metrics import hamming_loss\n",
    "print('Hamming Loss: ', round(hamming_loss(y_test, prediction_NB),2))\n",
    "#Check for precision, recall, f1-score.\n",
    "print(classification_report(y_test, prediction_NB, zero_division=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy Score:  0.7834344079157357\n",
      "Hamming Loss:  0.06\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.87      0.66      0.75      1857\n",
      "           1       0.81      0.83      0.82      1826\n",
      "           2       0.83      0.75      0.79      1919\n",
      "           3       0.77      0.80      0.78      1831\n",
      "           4       0.84      0.84      0.84      1791\n",
      "           5       0.98      0.65      0.78      1754\n",
      "\n",
      "   micro avg       0.84      0.76      0.80     10978\n",
      "   macro avg       0.85      0.76      0.79     10978\n",
      "weighted avg       0.85      0.76      0.79     10978\n",
      " samples avg       0.66      0.66      0.66     10978\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Wing hong\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1471: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in samples with no predicted labels. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "c:\\Users\\Wing hong\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1471: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in samples with no true labels. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    }
   ],
   "source": [
    "#Train for SVM\n",
    "SVM = MultiOutputClassifier(SVC()).fit(X_train_tfidf, y_train)\n",
    "prediction_SVM = SVM.predict(X_test_tfidf)\n",
    "prediction_SVM\n",
    "#Check for accuracy.\n",
    "from sklearn.metrics import accuracy_score\n",
    "print('Accuracy Score: ', accuracy_score(y_test, prediction_SVM))\n",
    "#Check for hamming loss\n",
    "from sklearn.metrics import hamming_loss\n",
    "print('Hamming Loss: ', round(hamming_loss(y_test, prediction_SVM),2))\n",
    "#Check for precision, recall, f1-score.\n",
    "print(classification_report(y_test, prediction_SVM))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy Score:  0.779684008937121\n",
      "Hamming Loss:  0.06\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.80      0.66      0.72      1857\n",
      "           1       0.79      0.83      0.81      1826\n",
      "           2       0.80      0.75      0.78      1919\n",
      "           3       0.77      0.78      0.77      1831\n",
      "           4       0.80      0.84      0.82      1791\n",
      "           5       0.86      0.66      0.75      1754\n",
      "\n",
      "   micro avg       0.80      0.76      0.78     10978\n",
      "   macro avg       0.80      0.76      0.78     10978\n",
      "weighted avg       0.80      0.76      0.78     10978\n",
      " samples avg       0.84      0.79      0.94     10978\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Train for kNN\n",
    "kNN = MultiOutputClassifier(KNeighborsClassifier()).fit(X_train_tfidf, y_train)\n",
    "prediction_kNN = kNN.predict(X_test_tfidf)\n",
    "prediction_kNN\n",
    "#Check for accuracy.\n",
    "from sklearn.metrics import accuracy_score\n",
    "print('Accuracy Score: ', accuracy_score(y_test, prediction_kNN))\n",
    "#Check for hamming loss\n",
    "from sklearn.metrics import hamming_loss\n",
    "print('Hamming Loss: ', round(hamming_loss(y_test, prediction_kNN),2))\n",
    "#Check for precision, recall, f1-score.\n",
    "print(classification_report(y_test, prediction_kNN, zero_division=1))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
